{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "d320f90f432c331afea02ef66fcddb59448ea200",
        "pycharm": {}
      },
      "source": [
        "# Approach\n",
        "\n",
        "* Firstly a convolutional neural network is used to segment the image, using the bounding boxes directly as a mask. \n",
        "* Secondly connected components is used to separate multiple areas of predicted pneumonia.\n",
        "* Finally a bounding box is simply drawn around every connected component.\n",
        "\n",
        "# Network\n",
        "\n",
        "* The network consists of a number of residual blocks with convolutions and downsampling blocks with max pooling.\n",
        "* At the end of the network a single upsampling layer converts the output to the same shape as the input.\n",
        "\n",
        "As the input to the network is 256 by 256 (instead of the original 1024 by 1024) and the network downsamples a number of times without any meaningful upsampling (the final upsampling is just to match in 256 by 256 mask) the final prediction is very crude. If the network downsamples 4 times the final bounding boxes can only change with at least 16 pixels."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "collapsed": true,
        "pycharm": {
          "is_executing": false
        }
      },
      "outputs": [],
      "source": "import os\nimport csv\nimport random\nimport pydicom\nimport numpy as np\nimport pandas as pd\nimport pickle\nfrom skimage import measure\nfrom skimage.transform import resize\n\nimport tensorflow as tf\nfrom tensorflow import keras\n\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as patches"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "8d58cefae21c951e077c02c1a989d020f18465cc",
        "pycharm": {}
      },
      "source": [
        "# Load pneumonia locations\n",
        "\n",
        "Table contains [filename : pneumonia location] pairs per row. \n",
        "* If a filename contains multiple pneumonia, the table contains multiple rows with the same filename but different pneumonia locations. \n",
        "* If a filename contains no pneumonia it contains a single row with an empty pneumonia location.\n",
        "\n",
        "The code below loads the table and transforms it into a dictionary. \n",
        "* The dictionary uses the filename as key and a list of pneumonia locations in that filename as value. \n",
        "* If a filename is not present in the dictionary it means that it contains no pneumonia."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "trusted": true,
        "_uuid": "e08496a85ef9b0823595c3745d2677c6e84b6a3a",
        "collapsed": true,
        "pycharm": {
          "is_executing": false
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "text": [
            "[\u0027test_model.py\u0027, \u0027.DS_Store\u0027, \u0027models.py\u0027, \u0027requirements.txt\u0027, \u0027train_model.py\u0027, \u0027input\u0027, \u0027kernel (1).ipynb\u0027, \u0027cnn\u0027, \u0027venv\u0027, \u0027separate_train_data.py\u0027, \u0027.idea\u0027]\n"
          ],
          "output_type": "stream"
        }
      ],
      "source": "def load_pneumonia_locations(path\u003d\u0027./input/stage_2_train_labels.csv\u0027):\n    \"\"\"\n    loads all pneumonia bounding boxes from csv into dict.\n    :param path: the path to the train labels csv\n    :return: pneumonia_locations: a dict of filename: list of bounding boxes\n    \"\"\"\n    pneumonia_locations \u003d {}\n    with open(os.path.join(path), mode\u003d\u0027r\u0027) as labels:\n        reader \u003d csv.reader(labels)\n        # skip header\n        next(reader, None)\n        for rows in reader:\n            filename \u003d rows[0]\n            location \u003d rows[1:5]\n            pneumonia \u003d rows[5]\n            if pneumonia \u003d\u003d \u00271\u0027:\n                location \u003d [int(float(i)) for i in location]\n                if filename in pneumonia_locations:\n                    pneumonia_locations[filename].append(location)\n                else:\n                    pneumonia_locations[filename] \u003d [location]\n    return pneumonia_locations\n\n\npneumonia_locations \u003d load_pneumonia_locations()\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "5a4fa6da9833fd7cbd476d19584ba35695b38ddb",
        "pycharm": {}
      },
      "source": [
        "# Load filenames"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "trusted": true,
        "_uuid": "ccd0b0d52cafd125558ed5560a9cc8fa15760bc5",
        "collapsed": false,
        "pycharm": {
          "is_executing": false
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "text": [
            "Loading 20547 train files\nLoading 5137 validation files\n"
          ],
          "output_type": "stream"
        }
      ],
      "source": "def load_file_names(path\u003d\u0027./input/stage_2_train_images\u0027, train_ratio\u003d0.8):\n    filenames \u003d os.listdir(path)\n    random.shuffle(filenames)\n    \n    train_files_count \u003d int(train_ratio*len(filenames))\n    train_filenames \u003d filenames[:train_files_count]\n    validation_filenames \u003d filenames[train_files_count:]\n    print(\u0027Loading {} train files\u0027.format(len(train_filenames)))\n    print(\u0027Loading {} validation files\u0027.format(len(validation_filenames)))\n    return train_filenames, validation_filenames\n\n\ntrain_filenames, validation_filenames \u003d load_file_names()"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "276b80f59fa9acdfd9307d74cee5f705cd6aa5b6",
        "pycharm": {}
      },
      "source": [
        " # Generator\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "trusted": true,
        "_uuid": "86b3f780a03cddda78c6adfde461d6ff8dad5672",
        "collapsed": true,
        "pycharm": {
          "is_executing": false
        }
      },
      "outputs": [],
      "source": "class Generator(keras.utils.Sequence):\n    \n    def __init__(self, folder, filenames, pneumonia_locations\u003dNone, batch_size\u003d32, image_size\u003d256, shuffle\u003dTrue, predict\u003dFalse):\n        \"\"\"\n        :str folder: folder the training/test images are in\n        :list(str) filenames: used images filenames \n        :dict(filename: list((x_coordinate, y_coordinate, width, height)) pneumonia_locations: bounding boxes\n        :int batch_size: \n        :int image_size: \n        :bool shuffle: whether images should be shuffled, do this for training\n        :bool predict: False if training, True if predicting\n        \"\"\"\n        self.folder \u003d folder\n        self.filenames \u003d filenames\n        self.pneumonia_locations \u003d pneumonia_locations\n        self.batch_size \u003d batch_size\n        self.image_size \u003d image_size\n        self.shuffle \u003d shuffle\n        self.predict \u003d predict\n        self.on_epoch_end()\n        \n    def __load__(self, filename):\n        img \u003d pydicom.dcmread(os.path.join(self.folder, filename)).pixel_array\n\n        # create mask with 1\u0027s at pneumonia bounding box\n        msk \u003d np.zeros(img.shape)\n        filename \u003d filename.split(\u0027.\u0027)[0]\n        if filename in self.pneumonia_locations:\n            for location in self.pneumonia_locations[filename]:\n                x, y, w, h \u003d location\n                msk[y:y+h, x:x+w] \u003d 1\n                \n        img \u003d resize(img, (self.image_size, self.image_size), mode\u003d\u0027reflect\u0027)\n        msk \u003d resize(msk, (self.image_size, self.image_size), mode\u003d\u0027reflect\u0027) \u003e 0.5\n        \n        img \u003d np.expand_dims(img, -1)\n        msk \u003d np.expand_dims(msk, -1)\n        return img, msk\n    \n    def __loadpredict__(self, filename):\n        img \u003d pydicom.dcmread(os.path.join(self.folder, filename)).pixel_array\n        img \u003d resize(img, (self.image_size, self.image_size), mode\u003d\u0027reflect\u0027)\n        img \u003d np.expand_dims(img, -1)\n        return img\n        \n    def __getitem__(self, index):\n        filenames \u003d self.filenames[index*self.batch_size:(index+1)*self.batch_size]\n        \n        if self.predict:\n            imgs \u003d [self.__loadpredict__(filename) for filename in filenames]\n            imgs \u003d np.array(imgs)\n            return imgs, filenames\n        \n        else:\n            items \u003d [self.__load__(filename) for filename in filenames]\n            imgs, msks \u003d zip(*items)\n            imgs \u003d np.array(imgs)\n            msks \u003d np.array(msks)\n            return imgs, msks\n        \n    def on_epoch_end(self):\n        if self.shuffle:\n            random.shuffle(self.filenames)\n        \n    def __len__(self):\n        if self.predict:\n            return int(np.ceil(len(self.filenames) / self.batch_size))\n        else:\n            return int(len(self.filenames) / self.batch_size)"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "ebc622a4b406354cc4ef28801eab72346a724d8b",
        "pycharm": {}
      },
      "source": [
        "# Network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "trusted": true,
        "_uuid": "9fc2b108689637a6037b48ebab3f7659b8704bf9",
        "collapsed": true,
        "pycharm": {
          "is_executing": false
        }
      },
      "outputs": [],
      "source": "def downsample(n_outputs, inputs):\n    x \u003d keras.layers.BatchNormalization(momentum\u003d0.9)(inputs)\n    x \u003d keras.layers.LeakyReLU(0)(x)\n    x \u003d keras.layers.Conv2D(n_outputs, 1, padding\u003d\u0027same\u0027, use_bias\u003dFalse)(x)\n    x \u003d keras.layers.MaxPool2D(2)(x)\n    return x\n\n\ndef resblock(n_outputs, inputs):\n    x \u003d keras.layers.BatchNormalization(momentum\u003d0.9)(inputs)\n    x \u003d keras.layers.LeakyReLU(0)(x)\n    x \u003d keras.layers.Conv2D(n_outputs, 3, padding\u003d\u0027same\u0027, use_bias\u003dFalse)(x)\n    x \u003d keras.layers.BatchNormalization(momentum\u003d0.9)(x)\n    x \u003d keras.layers.LeakyReLU(0)(x)\n    x \u003d keras.layers.Conv2D(n_outputs, 3, padding\u003d\u0027same\u0027, use_bias\u003dFalse)(x)\n    return keras.layers.add([x, inputs])\n\n\ndef create_model(input_size, n_outputs, n_blocks\u003d4, depth\u003d3):\n    inputs \u003d keras.Input(shape\u003d(input_size, input_size, 1))\n    x \u003d keras.layers.Conv2D(n_outputs, 3, padding\u003d\u0027same\u0027, use_bias\u003dFalse)(inputs)\n    for d in range(depth):\n        n_outputs \u003d n_outputs * 2\n        x \u003d downsample(n_outputs, x)\n        for b in range(n_blocks):\n            x \u003d resblock(n_outputs, x)\n\n    x \u003d keras.layers.BatchNormalization(momentum\u003d0.9)(x)\n    x \u003d keras.layers.LeakyReLU(0)(x)\n    x \u003d keras.layers.Conv2D(1, 1, activation\u003d\u0027sigmoid\u0027)(x)\n    outputs \u003d keras.layers.UpSampling2D(2**depth)(x)\n    model \u003d keras.Model(inputs\u003dinputs, outputs\u003doutputs)\n    return model"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "bee3bc9363e9c1829eadf17da7a67fbd1a6a369e",
        "pycharm": {}
      },
      "source": [
        "# Train network\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "trusted": true,
        "_uuid": "4369be30f61440eb6858d57829fa541c4ee893bf",
        "collapsed": false,
        "pycharm": {
          "is_executing": false
        }
      },
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "When passing a list as loss, it should have one entry per model outputs. The model has 1 outputs, but you passed loss\u003d[\u003cfunction jaccard_loss at 0x123f1ac80\u003e, \u003cfunction binary_crossentropy at 0x118d95e60\u003e]",
          "traceback": [
            "\u001b[0;31m\u001b[0m",
            "\u001b[0;31mValueError\u001b[0mTraceback (most recent call last)",
            "\u001b[0;32m\u003cipython-input-25-96ce51c4a96a\u003e\u001b[0m in \u001b[0;36m\u003cmodule\u003e\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m model.compile(optimizer\u003d\u0027adam\u0027,\n\u001b[1;32m     21\u001b[0m               \u001b[0mloss\u001b[0m\u001b[0;34m\u003d\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mjaccard_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary_crossentropy\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---\u003e 22\u001b[0;31m               metrics\u003d[\u0027accuracy\u0027, mean_iou])\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/Users/martinborgt/development/pneumonia/venv/lib/python2.7/site-packages/tensorflow/python/training/checkpointable/base.pyc\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    362\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setattr_tracking\u001b[0m \u001b[0;34m\u003d\u001b[0m \u001b[0mFalse\u001b[0m  \u001b[0;31m# pylint: disable\u003dprotected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    363\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--\u003e 364\u001b[0;31m       \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    365\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    366\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setattr_tracking\u001b[0m \u001b[0;34m\u003d\u001b[0m \u001b[0mprevious_value\u001b[0m  \u001b[0;31m# pylint: disable\u003dprotected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/Users/martinborgt/development/pneumonia/venv/lib/python2.7/site-packages/tensorflow/python/keras/engine/training.pyc\u001b[0m in \u001b[0;36mcompile\u001b[0;34m(self, optimizer, loss, metrics, loss_weights, sample_weight_mode, weighted_metrics, target_tensors, **kwargs)\u001b[0m\n\u001b[1;32m    228\u001b[0m                          \u001b[0;34m\u0027it should have one entry per model outputs. \u0027\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m                          \u001b[0;34m\u0027The model has \u0027\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--\u003e 230\u001b[0;31m                          \u0027 outputs, but you passed loss\u003d\u0027 + str(loss))\n\u001b[0m\u001b[1;32m    231\u001b[0m       \u001b[0mloss_functions\u001b[0m \u001b[0;34m\u003d\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: When passing a list as loss, it should have one entry per model outputs. The model has 1 outputs, but you passed loss\u003d[\u003cfunction jaccard_loss at 0x123f1ac80\u003e, \u003cfunction binary_crossentropy at 0x118d95e60\u003e]"
          ],
          "output_type": "error"
        }
      ],
      "source": "def jaccard_loss(y_true, y_pred, smoothing\u003d1e-12):\n    y_true \u003d tf.reshape(y_true, [-1])\n    y_pred \u003d tf.reshape(y_pred, [-1])\n    intersection \u003d tf.reduce_sum(y_true * y_pred)\n    sum_ \u003d tf.reduce_sum(y_true + y_pred)\n    score \u003d (smoothing + intersection ) / (sum_ - intersection + smoothing)\n    return 1 - score\n\n\ndef jaccard_bce_loss(y_true, y_pred):\n    y_true \u003d tf.reshape(y_true, [-1])\n    y_pred \u003d tf.reshape(y_pred, [-1])\n    return 0.5 * keras.losses.binary_crossentropy(y_true, y_pred) + 0.5 * jaccard_loss(y_true, y_pred)\n\n\ndef mean_iou(y_true, y_pred):\n    y_pred \u003d tf.round(y_pred)\n    intersect \u003d tf.reduce_sum(y_true * y_pred, axis\u003d[1, 2, 3])\n    union \u003d tf.reduce_sum(y_true, axis\u003d[1, 2, 3]) + tf.reduce_sum(y_pred, axis\u003d[1, 2, 3])\n    smooth \u003d tf.ones(tf.shape(intersect))\n    return tf.reduce_mean((intersect + smooth) / (union - intersect + smooth))\n\n\n# create network and compiler\nmodel \u003d create_model(input_size\u003d256, n_outputs\u003d32, n_blocks\u003d4, depth\u003d3)\nmodel.compile(optimizer\u003d\u0027adam\u0027,\n              loss\u003djaccard_bce_loss,\n              metrics\u003d[\u0027accuracy\u0027, mean_iou])\n\ntry:\n    model.load_weights(\u0027pneumonia_cnn.h5\u0027)\nexcept OSError:\n    print(\"Could not load model, starting from random.\")\n\ndef cosine_annealing(x):\n    base_learning_rate \u003d 0.001\n    epochs \u003d 25\n    return base_learning_rate*(np.cos(np.pi*x/epochs)+1.)/2\n\n\nlearning_rate \u003d tf.keras.callbacks.LearningRateScheduler(cosine_annealing)\n\nfolder \u003d \u0027../input/stage_2_train_images\u0027\ntrain_gen \u003d Generator(folder, train_filenames, pneumonia_locations, batch_size\u003d32, image_size\u003d256, shuffle\u003dTrue, predict\u003dFalse)\nvalid_gen \u003d Generator(folder, validation_filenames, pneumonia_locations, batch_size\u003d32, image_size\u003d256, shuffle\u003dFalse, predict\u003dFalse)\n\nhistory \u003d model.fit_generator(\n    train_gen, \n    validation_data\u003dvalid_gen, \n    callbacks\u003d[learning_rate], \n    epochs\u003d1, \n    workers\u003d4, \n    use_multiprocessing\u003dTrue\n)\n\nmodel.save_weights(\u0027pneumonia_cnn.h5\u0027)\nprint(\"model saved.\")"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true,
        "_uuid": "fbbf7546d396560d21b7af17d4c959713f425d8e",
        "collapsed": true,
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "def add_bounding_box(axidx, axarr, msk, bounding_box_threshold\u003d0.5, edgecolor\u003d\u0027b\u0027):\n",
        "    comp \u003d msk[:, :, 0] \u003e bounding_box_threshold\n",
        "    comp \u003d measure.label(comp)\n",
        "    for region in measure.regionprops(comp):\n",
        "        y, x, y2, x2 \u003d region.bbox\n",
        "        height \u003d y2 - y\n",
        "        width \u003d x2 - x\n",
        "        axarr[axidx].add_patch(patches.Rectangle((x, y), width, height, linewidth\u003d2, edgecolor\u003dedgecolor, facecolor\u003d\u0027none\u0027))\n",
        "\n",
        "\n",
        "def validate_model(model, validation_generator, bounding_box_threshold\u003d0.5):\n",
        "    for imgs, msks in validation_generator:\n",
        "        preds \u003d model.predict(imgs)\n",
        "        f, axarr \u003d plt.subplots(4, 8, figsize\u003d(20,15))\n",
        "        axarr \u003d axarr.ravel()\n",
        "        axidx \u003d 0\n",
        "        \n",
        "        for img, msk, pred in zip(imgs, msks, preds):\n",
        "            axarr[axidx].imshow(img[:, :, 0])\n",
        "            add_bounding_box(axidx, axarr, msk, bounding_box_threshold)\n",
        "            add_bounding_box(axidx, axarr, pred, bounding_box_threshold, edgecolor\u003d\u0027r\u0027)\n",
        "            axidx +\u003d 1\n",
        "        plt.show()\n",
        "        break\n",
        "        \n",
        "        \n",
        "validate_model(model, valid_gen)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "25dc7ad848190bb37349a80f96ed2bdb5c3821b0",
        "pycharm": {}
      },
      "source": [
        "# Predict test images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true,
        "_uuid": "2c9277e4ec9f12712dd690002c540b396278c504",
        "scrolled": false,
        "collapsed": true,
        "pycharm": {}
      },
      "outputs": [],
      "source": "folder \u003d \u0027../input/stage_2_test_images\u0027\ntest_filenames \u003d os.listdir(folder)\nprint(\u0027{} test samples:\u0027.format(len(test_filenames)))\n\ntest_gen \u003d Generator(folder, test_filenames, None, batch_size\u003d25, image_size\u003d256, shuffle\u003dFalse, predict\u003dTrue)\n\nsubmission_dict \u003d {}\nfor imgs, filenames in test_gen:\n    preds \u003d model.predict(imgs)\n    for pred, filename in zip(preds, filenames):\n        pred \u003d resize(pred, (1024, 1024), mode\u003d\u0027reflect\u0027)\n        comp \u003d pred[:, :, 0] \u003e 0.5\n        comp \u003d measure.label(comp)\n        predictionString \u003d \u0027\u0027\n        for region in measure.regionprops(comp):\n            y, x, y2, x2 \u003d region.bbox\n            height \u003d y2 - y\n            width \u003d x2 - x\n            confidence \u003d np.mean(pred[y:y+height, x:x+width])\n            predictionString +\u003d \u0027{confidence} {x} {y} {width} {height} \u0027.format(confidence\u003dconfidence, x\u003dx, y\u003dy, width\u003dwidth, height\u003dheight)\n\n        filename \u003d filename.split(\u0027.\u0027)[0]\n        submission_dict[filename] \u003d predictionString\n    if len(submission_dict) \u003e\u003d len(test_filenames):\n        break\n\n\nsub \u003d pd.DataFrame.from_dict(submission_dict, orient\u003d\u0027index\u0027)\nsub.index.names \u003d [\u0027patientId\u0027]\nsub.columns \u003d [\u0027PredictionString\u0027]\nsub.to_csv(\u0027submission.csv\u0027)"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "6b7bbc372fddde87f95660b79482d58137ff4ff3",
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}